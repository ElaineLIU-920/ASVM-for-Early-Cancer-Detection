{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from mlxtend.evaluate import BootstrapOutOfBag\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from pandas import *\n",
    "from scipy import stats\n",
    "\n",
    "xl = pd.ExcelFile('data2.xlsx')\n",
    "xl.sheet_names # we'll take 7th\n",
    "dfs = {sheet: xl.parse(sheet) for sheet in xl.sheet_names}\n",
    "data1 = dfs['7']\n",
    "data2 = dfs['1'].loc[:,['Patient','Age at Diagnosis']].drop([554]).drop_duplicates()\n",
    "# import datas/et1\n",
    "data3 = pd.read_csv('data1.csv')\n",
    "\n",
    "combined_data = data1.set_index('Patient').join(data2.set_index('Patient')).join(data3.set_index('Patient'))\n",
    "\n",
    "combined_data['label'] = (combined_data['Patient Type'] == 'Healthy').astype(int)\n",
    "combined_data = combined_data.drop(['Patient Type'],axis=1)\n",
    "print('The number of samples and features are %d and %d, respectively'%(combined_data.shape[0],combined_data.shape[1]))\n",
    "\n",
    "\n",
    "x = combined_data.iloc[:, 0:44]\n",
    "x[isnan(x)] = 0\n",
    "y=combined_data.iloc[:,44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def Score_632plus(clf,x,y,x_train,y_train,x_test,y_test): \n",
    "    y_score_test = clf.predict_proba(x_test)[:,1]\n",
    "    y_score_train = clf.predict_proba(x_train)[:,1]  \n",
    "    \n",
    "    # Compute error based on auc\n",
    "    E_resub =  metrics.brier_score_loss(y_train, y_score_train)\n",
    "    E_loop = metrics.brier_score_loss(y_test, y_score_test)\n",
    "    # compute no-information-rate\n",
    "    y_pred = clf.predict(x)\n",
    "    length = len(y_pred)\n",
    "    gamma = sum(y==0)/length*(1-sum(y_pred==0)/length)+sum(y==1)/length*(1-sum(y_pred==1)/length)\n",
    "    \n",
    "    E_632 = 0.368*E_resub + 0.632*E_loop\n",
    "    \n",
    "    # compute R\n",
    "    if gamma > E_resub:\n",
    "        R = (E_loop - E_resub) / (gamma - E_resub)\n",
    "    else:\n",
    "        R = 0\n",
    "        E_resub = gamma\n",
    "    \n",
    "    # compute weight and adjusted Score\n",
    "    E_632plus = E_632 + (E_resub - E_loop)*(0.368*0.632*R/(1-0.368*R))\n",
    "    return E_632plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SFLA_SVM(x_train, y_train,x_traincv, y_traincv,x_testcv, y_testcv,kernel,C,gamma=False,degree=False,coef0=False):\n",
    "    clf = svm.SVC(C=C, kernel=kernel,gamma=gamma, coef0=coef0, probability=True,random_state=920).fit(x_traincv, y_traincv)\n",
    "    bscore = -Score_632plus(clf,x_train, y_train,x_traincv, y_traincv,x_testcv, y_testcv)\n",
    "    return bscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SFLA_SVM_CV(x_train, y_train,n,kernel,C,gamma=False,degree=False,coef0=False):\n",
    "    '''\n",
    "    n: number of splits for k-fold\n",
    "    \n",
    "    '''\n",
    "    KF = KFold(n_splits=n,shuffle=True, random_state=920)\n",
    "    f = []\n",
    "    for train_indexcv,test_indexcv in KF.split(x_train):\n",
    "        x_traincv, x_testcv = x_train.iloc[train_indexcv][:], x_train.iloc[test_indexcv][:]\n",
    "        y_traincv, y_testcv = y_train.iloc[train_indexcv][:], y_train.iloc[test_indexcv][:]\n",
    "        fq = SFLA_SVM(x_train, y_train,x_traincv, y_traincv,x_testcv, y_testcv,kernel,C,gamma) \n",
    "        f.append(fq) \n",
    "    f = mean(f)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SFLA_RBF(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,x_train,y_train):\n",
    "    '''\n",
    "    num_parameter: int, number of parameter to optimize\n",
    "    \n",
    "    num_global: int, the maximum number of global iterations\n",
    "    \n",
    "    num_local: int, the maximum number of local iterations\n",
    "    \n",
    "    m : int, the number of memeplexes\n",
    "    \n",
    "    n : int, the number of frogs in each memeplex\n",
    "    \n",
    "    q : int, the number of frogs in submemeplex\n",
    "    \n",
    "    n1:  number of splits for cross validation for inner loop\n",
    "    \n",
    "    rangeC: list, float, range of parameter C,eg.[10**-2, 10**2]\n",
    "    \n",
    "    rangeGamma: list, float, range of parameter Gamma,eg.[10**-6, 1]\n",
    "\n",
    "    x_train: feature\n",
    "\n",
    "    y_train: lable\n",
    "\n",
    "    '''\n",
    "\n",
    "    #--- Step 0--Initialize parameters ---#\n",
    "    sizeC = 2\n",
    "    sizeGamma = 2\n",
    "    max_step =  [(rangeC[1]-rangeC[0])/sizeC,(rangeGamma[1]-rangeGamma[0])/sizeGamma]# maximum step size\n",
    "    \n",
    "    #--- Step 1--Generate initial population ---#\n",
    "    frogC = 10**random.uniform(log10(rangeC[0]),log10(rangeC[1]),m*n)\n",
    "    frogGamma = 10**random.uniform(log10(rangeGamma[0]),log10(rangeGamma[1]),m*n)\n",
    "    frog = c_[frogC,frogGamma]\n",
    "\n",
    "    # Compute the performance value for each frog on validation data #\n",
    "    KF = KFold(n_splits=n1,shuffle=True, random_state=920)\n",
    "    f = zeros((m*n,n1))\n",
    "    j = 0\n",
    "    for train_indexcv,test_indexcv in KF.split(x_train):\n",
    "        x_traincv, x_testcv = x_train.iloc[train_indexcv][:], x_train.iloc[test_indexcv][:]\n",
    "        y_traincv, y_testcv = y_train.iloc[train_indexcv][:], y_train.iloc[test_indexcv][:]\n",
    "        for i in range(m*n):\n",
    "            f[i,j] = SFLA_SVM(x_train, y_train,x_traincv, y_traincv,x_testcv, y_testcv,kernel,frog[i,0],frog[i,1])\n",
    "        j+=1\n",
    "    f = f.mean(axis=1)\n",
    "    f_parameter = c_[f,frog]\n",
    "\n",
    "    #--- Step 2--Rank frogs ---#\n",
    "    f_parameter = f_parameter[argsort(f_parameter[:,0])[::-1]]\n",
    "\n",
    "\n",
    "    #######--- Global search start---######\n",
    "    i_global = 0\n",
    "    flag = 0\n",
    "    fBest_iteration = f_parameter[0,0]\n",
    "    weights = [2*(n+1-j)/(n*(n+1)) for j in range(1,n+1)] # weights of ranked frogs in each memeplex\n",
    "    while i_global < num_global:\n",
    "        frog_gb = f_parameter[0,0] # mark the global best frog      \n",
    "        #--- Step 3--Partition frogs into memeplexes ---#\n",
    "        memeplexes = zeros((m,n,num_parameter+1)) # [memeplexes, frog in memeplex,[f,C,Gamma] ]\n",
    "        for i in range(m):\n",
    "            memeplexes[i] = f_parameter[linspace(i,m*n+i,num=n,endpoint=False,dtype=int)]\n",
    "\n",
    "        #######--- Local search start---######\n",
    "        #--- Step 4--Memetic evolution within each memeplex ---#\n",
    "        im = 0 # the number of memeplexes that have been optimized\n",
    "        while im < m:\n",
    "            i_local = 0 # counts the number of local evolutionary steps in each memeplex\n",
    "            while i_local < num_local:\n",
    "\n",
    "                #--- Construct a submemeplex ---#\n",
    "                rValue = random.random(n)*weights # random value with probability weights\n",
    "                subindex = sort(argsort(rValue)[::-1][0:q]) # index of selected frogs in memeplex \n",
    "                submemeplex = memeplexes[im][subindex] # form submemeplex\n",
    "\n",
    "                #--- Improve the worst frog's position ---#\n",
    "                # Learn from local best Pb #\n",
    "                Pb = submemeplex[0] # mark the best frog in submemeplex\n",
    "                Pw = submemeplex[q-1] # mark the worst frog in memeplex\n",
    "                S = (Pb-Pw)[1:]*(Pb-Pw)[0] \n",
    "                Uq = Pw[1:]+S\n",
    "                # Check feasible space and the performance #\n",
    "                if (rangeC[0] <= Uq[0] <=rangeC[1]) and (rangeGamma[0] <= Uq[1] <=rangeGamma[1]): # check feasible space\n",
    "                    fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])\n",
    "                    if fq < Pw[0]: # if no improvement of performance,learn from global best randomly #\n",
    "                        S = random.random(num_parameter)*(frog_gb-Pw)[1:]\n",
    "                        for i in range(num_parameter):\n",
    "                            if S[i] > 0:\n",
    "                                S[i] = min(S[i],max_step[i])\n",
    "                            else:\n",
    "                                S[i] = min(S[i],-max_step[i])\n",
    "                        Uq = Pw[1:]+S\n",
    "                        if (rangeC[0] <= Uq[0] <=rangeC[1]) and (rangeGamma[0] <= Uq[1] <=rangeGamma[1]): # check feasible space\n",
    "                            fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])\n",
    "                            if fq < Pw[0]: # if no improvement of performance, randomly generate a new frog\n",
    "                                Uq = [10**random.uniform(log10(rangeC[0]),log10(rangeC[1])),10**random.uniform(log10(rangeGamma[0]),log10(rangeGamma[1]))]\n",
    "                                fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])\n",
    "                        else: # if not in the feasible space, randomly generate a new frog\n",
    "                            Uq = [10**random.uniform(log10(rangeC[0]),log10(rangeC[1])), 10**random.uniform(log10(rangeGamma[0]),log10(rangeGamma[1]))]\n",
    "                            fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])            \n",
    "                else: # if not in the feasible space, learn from global best randomly \n",
    "                    S = random.random(num_parameter)*(frog_gb-Pw)[1:]\n",
    "                    for i in range(num_parameter):\n",
    "                        if S[i] > 0:\n",
    "                            S[i] = min(S[i],max_step[i])\n",
    "                        else:\n",
    "                            S[i] = min(S[i],-max_step[i])\n",
    "                    Uq = Pw[1:]+S\n",
    "                    if (rangeC[0] <= Uq[0] <=rangeC[1]) and (rangeGamma[0] <= Uq[1] <=rangeGamma[1]): # check feasible space\n",
    "                        fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])\n",
    "                        if fq < Pw[0]: # if no improvement of performance, randomly generate a new frog\n",
    "                            Uq = [10**random.uniform(log10(rangeC[0]),log10(rangeC[1])), 10**random.uniform(log10(rangeGamma[0]),log10(rangeGamma[1]))]\n",
    "                            fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])\n",
    "                    else: # if not in the feasible space, randomly generate a new frog\n",
    "                        Uq = [10**random.uniform(log10(rangeC[0]),log10(rangeC[1])), 10**random.uniform(log10(rangeGamma[0]),log10(rangeGamma[1]))]\n",
    "                        fq = SFLA_SVM_CV(x_train, y_train,n1,kernel,Uq[0],Uq[1])\n",
    "\n",
    "                #--- Upgrade the memeplex ---# \n",
    "                memeplexes[im][subindex[q-1]] = r_[fq,Uq]\n",
    "                memeplexes[im] =  memeplexes[im][argsort( memeplexes[im][:,0])[::-1]]            \n",
    "\n",
    "                i_local += 1\n",
    "\n",
    "            im += 1\n",
    "        #######--- Local search end---######    \n",
    "\n",
    "        #--- Step 5--Shuffle memeplexes ---#\n",
    "        f_parameter =  memeplexes.reshape(m*n,num_parameter+1)\n",
    "        f_parameter = f_parameter[argsort(f_parameter[:,0])[::-1]]\n",
    "\n",
    "\n",
    "        i_global += 1\n",
    "\n",
    "        #--- Step 6--Check convergence ---#\n",
    "        if f_parameter[0,0] > 0.99:\n",
    "            print('The program was terminated because it reached the optimization goal with f = %.3f' %f_parameter[0,0])\n",
    "            break\n",
    "#         if   abs(frog_gb - f_parameter[0,0])<10**-4:\n",
    "#             flag +=1\n",
    "#         if flag > 30:\n",
    "#             break\n",
    "        fBest_iteration = r_[fBest_iteration,f_parameter[0,0]] \n",
    "\n",
    "    #######--- Global search end---######\n",
    "        \n",
    "    return (f_parameter[0],fBest_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###--- ASVM 0.632+bootstrap---###\n",
    "\n",
    "##--- ASVM parameter---##\n",
    "start = time.process_time()\n",
    "num_parameter = 2# number of parameter to optimize\n",
    "num_global = 30# the maximum number of global iterations\n",
    "num_local = 20# the maximum number of local iterations\n",
    "m =4 # the number of memeplexes\n",
    "n = 8 # the number of frogs in each memeplex\n",
    "q = 5 # the number of frogs in submemeplex\n",
    "n1 = 10 # number of splits for inner loop\n",
    "kernel = 'rbf'\n",
    "rangeC = [10**-12, 10**12] # list, float, range of parameter C,eg.[10**-2, 10**2]\n",
    "rangeGamma = [10**-6, 1]\n",
    "\n",
    "oob = BootstrapOutOfBag(n_splits=50,random_seed = 920)\n",
    "Brier_split = []\n",
    "for train_index, test_index in oob.split(combined_data):\n",
    "    #---  Seperate traing set and test set ---#\n",
    "    x_train, x_test = x.iloc[train_index][:], x.iloc[test_index][:]\n",
    "    y_train = y.iloc[train_index][:]\n",
    "    \n",
    "    #---  Fill NaN age ---#\n",
    "    x_train[isnan(x_train)] = 0\n",
    "    x_test[isnan(x_test)] = 0\n",
    "    \n",
    "    ##---  optimize SVM with SFLA---##\n",
    "    f_parameter,fBest_iteration = SFLA_RBF(num_parameter,num_global,num_local,m,n,q,n1,kernel,rangeC,rangeGamma,x_train,y_train)\n",
    "    \n",
    "    ##---  creat and train the model ---##\n",
    "    clf = svm.SVC(kernel=kernel,C=f_parameter[1],gamma=f_parameter[2],probability=True,random_state=920)\n",
    "    clf.fit(x_train, y_train)\n",
    "    \n",
    "    ##---  Calculate Performance ---##\n",
    "    y_score_test = clf.predict_proba(x_test)[:,1]\n",
    "    y_score_train = clf.predict_proba(x_train)[:,1]  \n",
    "    \n",
    "    # Compute error based on auc\n",
    "    E_resub =  metrics.brier_score_loss(y_train, y_score_train)\n",
    "    E_loop = metrics.brier_score_loss(y[test_index], y_score_test)\n",
    "    \n",
    "#     # Compute brier_score\n",
    "#     E_resub = metrics.brier_score_loss(y_train, y_score_train)\n",
    "#     E_loop = metrics.brier_score_loss(y[test_index], y_score_test)\n",
    "    \n",
    "    # compute no-information-rate\n",
    "    y_pred = clf.predict(x)\n",
    "    length = len(y_pred)\n",
    "    gamma = sum(y==0)/length*(1-sum(y_pred==0)/length)+sum(y==1)/length*(1-sum(y_pred==1)/length)\n",
    "    \n",
    "    E_632 = 0.368*E_resub + 0.632*E_loop\n",
    "    \n",
    "    # compute R\n",
    "    if gamma > E_resub:\n",
    "        R = (E_loop - E_resub) / (gamma - E_resub)\n",
    "    else:\n",
    "        R = 0\n",
    "        E_resub = gamma\n",
    "    \n",
    "    # compute weight and adjusted Score\n",
    "#     weight = 0.632 / (1 - 0.368*R)\n",
    "    E_632plus = E_632 + (E_resub - E_loop)*(0.368*0.632*R/(1-0.368*R))\n",
    "    \n",
    "#     AUC_632plus = 1 - (weight*E_loop + (1. - weight)*E_resub)\n",
    "\n",
    "    Brier_split.append(E_632plus)\n",
    "    print(Brier_split)\n",
    "end = time.process_time()\n",
    "\n",
    "print('The ASVM algorithm takes '+str(end - start)+'seconds.\\n')\n",
    "print('Brier Score in each split:', Brier_split)\n",
    "Brier = mean(Brier_split)\n",
    "print('Average of Brier Score: %.4f' % (Brier))\n",
    "\n",
    "# Confidence interval\n",
    "lower = percentile(Brier_split, 2.5)\n",
    "upper = percentile(Brier_split, 97.5)\n",
    "print('95%% Confidence interval: [%.4f, %.4f]' % (lower, upper))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
